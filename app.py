import streamlit as st
import google.generativeai as genai
from langchain_google_genai import GoogleGenerativeAI
from PIL import Image, ImageDraw
from torchvision import transforms
from torchvision.models.detection import fasterrcnn_resnet50_fpn
import os
import torch
import pytesseract
from gtts import gTTS
import tempfile
import time
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Key setup
genai.configure(api_key="Gemini Api Key")

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # replace your path here#

# Page title
st.set_page_config(
    page_title="AI",
    layout="wide",
    page_icon="ğŸ¤–",
)

# Streamlit App Layout
st.markdown(
    """
    <style>
     .main-title {
        font-size: 48px;
        font-weight: bold;
        text-align: center;
        color: #555;
        margin-top: -20px;
     }
    .subtitle {
        font-size: 18px;
        color: #555;
        text-align: center;
        margin-bottom: 20px;
    }
    .feature-header {
        font-size: 24px;
        color: #333;
        font-weight: bold;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

st.markdown('<div class="main-title"> ğŸ‘ï¸â€ğŸ—¨ï¸Vision AIğŸ‘ï¸â€ğŸ—¨ï¸</div>', unsafe_allow_html=True)
st.markdown('<div class="subtitle">Transforming Lives with AI: Real-Time Scene Understanding, Object Detection, Personalized Assistance, and Converting text into audio output. </div>', unsafe_allow_html=True)

# Load Object Detection Model
@st.cache_resource
def load_object_detection_model():
    model = fasterrcnn_resnet50_fpn(pretrained=True)
    model.eval()
    return model

object_detection_model = load_object_detection_model()

def detect_objects(image, threshold=0.3, iou_threshold=0.5):
    # Transform image to tensor
    transform = transforms.Compose([transforms.ToTensor()])
    img_tensor = transform(image)
    predictions = object_detection_model([img_tensor])[0]
    keep = torch.ops.torchvision.nms(predictions['boxes'], predictions['scores'], iou_threshold)

    filtered_predictions = {
        'boxes': predictions['boxes'][keep],
        'labels': predictions['labels'][keep],
        'scores': predictions['scores'][keep],
    }
    return filtered_predictions

def draw_boxes(image, predictions, threshold=0.5):
    draw = ImageDraw.Draw(image)
    for label, box, score in zip(predictions['labels'], predictions['boxes'], predictions['scores']):
        if score > threshold:
            x1, y1, x2, y2 = box
            draw.rectangle([x1, y1, x2, y2], outline="yellow", width=5)
    return image

# Text extraction from image
def extract_text_from_image(uploaded_file):
    img = Image.open(uploaded_file)
    extracted_text = pytesseract.image_to_string(img)
    return extracted_text.strip() or "No text found in the image."

# Converting text to speech
def text_to_speech(text):
    tts = gTTS(text)
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp_file:
        tts.save(tmp_file.name)
        st.audio(tmp_file.name, format="audio/mp3")

# Converting image to bytes
def image_to_bytes(uploaded_file):
    bytes_data = uploaded_file.getvalue()
    return [{"mime_type": uploaded_file.type, "data": bytes_data}]

def get_assistance_response(input_prompt, image_data):
    # Define the system prompt
    system_prompt = """
    You are a specialized AI that provides accessibility assistance to visually impaired individuals. Your goal is to:
    1. Describing images in clear and simple language.
    2. Detecting objects and obstacles to help with navigation.
    3. Offering personalized suggestions based on the image content.
    4. Extracting and reading text from images clearly.
    """

    # Combine the system prompt with the user input prompt
    full_prompt = f"{system_prompt}\n{input_prompt}"
    model = genai.GenerativeModel("gemini-1.5-flash")
    response = model.generate_content([full_prompt, image_data[0]])
    return response.text

# UI Design
st.sidebar.image(r"C:\Users\mdimr\Downloads\RagModel_Paper\Image .jpg", use_container_width=True)
st.sidebar.header("Upload")
uploaded_file = st.sidebar.file_uploader("Upload an Image:", type=['jpg', 'jpeg', 'png', 'webp'])

# Display uploaded image on the main page (right-hand side)
if uploaded_file:
    st.image(uploaded_file, caption="Uploaded Image", use_container_width=True)

# Declaring the features
st.markdown("""
### Features
- ğŸï¸ **Scene Analysis**: Describe the content of an image in brief and detailed.
- ğŸš§ **Object Detection**: Highlight objects and obstacles for navigation.
- ğŸ¤– **Personalized Assistance**: Generate context-aware suggestions.
- ğŸ“ **Text-to-Speech**: Convert text into audio outputs.
""")

tab1, tab2, tab3, tab4 = st.tabs(["Scene Analysis", "Object Detection", "Assistance", "Text-to-Speech"])

# Scene Analysis
with tab1:
    st.subheader("ğŸï¸ Scene Analysis")
    if uploaded_file:
        with st.spinner("Analyzing Image..."):
            image_data = image_to_bytes(uploaded_file)
            user_prompt = "Describe this image in detail, briefly, with more text, clearly and concisely for visually impaired individuals."
            response = get_assistance_response(user_prompt, image_data)
            st.write(response)
            text_to_speech(response)

# Object Detection
with tab2:
    st.subheader("ğŸš§ Object Detection")
    if uploaded_file:
        try:
            image = Image.open(uploaded_file)
            predictions = detect_objects(image)
            if predictions:
                image_with_boxes = draw_boxes(image.copy(), predictions)
                st.image(image_with_boxes, caption="Objects Highlighted", use_container_width=True)
            else:
                st.write("No objects detected in the image.")
        except Exception as e:
            st.error(f"Error processing the image: {e}")

# Assistance Tab
with tab3:
    st.subheader("ğŸ¤– Personalized Assistance")
    if uploaded_file:
        st.image(uploaded_file, caption="Uploaded Image", use_container_width=True)
        with st.spinner("Analyzing for personalized assistance..."):
            image_data = image_to_bytes(uploaded_file)
            user_prompt = "Provide detailed assistance based on the uploaded image."
            response = get_assistance_response(user_prompt, image_data)
            st.write(response)
            text_to_speech(response)

# Text-to-Speech Tab
with tab4:
    st.subheader("ğŸ“ Text Extraction and Speech")
    if uploaded_file:
        text = extract_text_from_image(uploaded_file)
        st.write(f"Extracted Text: {text}")
        if text:
            text_to_speech(text)
